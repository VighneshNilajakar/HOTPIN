Highly Detailed Prompt Blueprint for the Hotpin Conversation Prototype AI Agent




I. Executive Summary: Hotpin Prototype Architectural Overview


The design of the Hotpin conversation-only prototype requires a rigorous approach to concurrency management, integrating high-performance, asynchronous web components with inherently blocking, CPU-intensive audio processing libraries. The resulting architecture is a unified system built on FastAPI and Uvicorn, utilizing WebSockets for real-time bidirectional communication, Groq for low-latency LLM interaction, and specialized worker processes for synchronous Speech-to-Text (STT) via Vosk and Text-to-Speech (TTS) via pyttsx3.


A. Project Scope and Concurrency Constraints


The system's non-functional requirement centers on minimizing perceived conversation latency, targeting sub-500ms for the critical path (STT  LLM  TTS). Achieving this goal is dependent on preventing the fundamental failure mode of Asynchronous Server Gateway Interface (ASGI) applications: event loop blockage.1 The FastAPI server, running on Uvicorn, uses an asyncio event loop to manage I/O-bound tasks concurrently. Operations such as network communication (WebSockets, Groq API calls) naturally fit this model and must be implemented using
async def and await keywords.1
However, the chosen STT (Vosk) and TTS (pyttsx3) components are synchronous and CPU-bound. If these blocking operations were executed directly within the async def context, they would suspend the main event loop for the duration of their execution, effectively freezing all other concurrent requests handled by that Uvicorn worker process.3 The architectural validation confirms that the solution must rely on strict concurrency separation.5


B. Core Technology Stack Mapping and Isolation Strategy


To ensure server responsiveness, all interactions with Vosk and pyttsx3 must be encapsulated within standard Python def functions. FastAPI recognizes these synchronous function definitions when they are called from an asynchronous endpoint, and intelligently delegates their execution to an internal thread pool.5 This mechanism ensures the responsiveness of the overall server by isolating the blocking work.3
The system architecture is summarized below, detailing the role and concurrency strategy for each component.
Table 1: Component Concurrency Mapping and Execution Strategy
Component
	Primary Function
	I/O Type
	FastAPI Handling Strategy
	Resource Management
	FastAPI/Uvicorn
	Server Orchestration
	ASGI/Async
	Main Event Loop
	Manages incoming connections and scheduling
	WebSocket
	Client Communication
	I/O-Bound
	async def endpoint (receive_bytes, send_bytes)
	Handles session buffering and byte streaming
	Vosk STT
	Transcription (Acoustic Model)
	CPU-Bound (Blocking)
	Sync def function executed in internal Thread Pool
	Global model loading, in-memory WAV encapsulation
	pyttsx3 TTS
	Audio Generation (Synthesis)
	CPU-Bound (Blocking)
	Sync def function executed in internal Thread Pool
	Temporary file I/O for output capture, isolated in thread
	Groq Client
	LLM Interaction (API Call)
	I/O-Bound
	httpx.AsyncClient (Explicit await)
	Application-scoped, connection-pooled client
	

C. Scaling and Parallelism for CPU-Bound Tasks


While moving blocking tasks to a thread pool addresses event loop blockage, the fundamental constraint of Python's Global Interpreter Lock (GIL) means that even within separate threads inside a single process, only one thread can execute Python byte code at a time.4 This limits true CPU parallelism for the Vosk and pyttsx3 tasks.
To achieve maximum concurrency and leverage multi-core processors for these CPU-heavy operations, the AI Agent must be instructed to deploy the FastAPI application using multiple Uvicorn worker processes.3 Each worker runs its own Python process with its own GIL, allowing simultaneous execution of Vosk and pyttsx3 tasks across different CPU cores. The directive for the AI Agent must therefore include the Uvicorn launch command specifying multiple workers (e.g.,
uvicorn main:app --workers 4).


II. The Real-Time WebSocket Communication Pipeline Design


The WebSocket endpoint is the critical interface, managing the session lifecycle and transporting high-throughput binary audio data in both directions.7 A clearly defined protocol is necessary to manage state handover and data type switching between text (JSON) and binary (PCM/WAV) formats.


A. Protocol Definition and Session Handover


Upon initial connection, the client and server must establish context. Since HTTP/WebSockets are connection-oriented but typically stateless in deployment, the client must explicitly provide a session identifier.
Table 2: Hotpin WebSocket Message Protocol Definition
Direction
	Type
	Data Format
	Payload Content
	Purpose
	Client  Server
	Text (JSON)
	{session_id: str}
	Initial handshake metadata
	Session identification for context retrieval
	Client  Server
	Binary
	Raw PCM (16-bit, 16kHz, mono)
	Continuous audio chunk bytes
	Real-time voice input stream
	Client  Server
	Text (JSON)
	{signal: "EOS"}
	End-of-speech signal
	Explicitly triggers transcription/LLM call
	Server  Client
	Binary
	WAV Chunks
	Synthesized audio output stream
	Real-time voice response
	The asynchronous WebSocket handler must first await a text message containing the session_id. All subsequent logic for that connection (including buffering) is keyed off this ID. The handling loop must be prepared to receive either binary audio data for buffering or a textual signal (e.g., "EOS") to terminate the current audio segment and initiate the conversational process.


B. Inbound Audio Stream Handling and PCM to WAV Encapsulation


The client is mandated to stream raw, 16-bit, 16kHz, mono PCM audio bytes. This format specification is crucial as it avoids complex, resource-intensive resampling on the server, ensuring compatibility with standard STT libraries like Vosk.8
The WebSocket handler must accumulate these incoming binary chunks into a session-specific io.BytesIO buffer.9 Once the End-Of-Speech (EOS) signal is received, this raw PCM buffer must be converted into a recognized audio container format for Vosk processing.
The conversion process must leverage Python’s standard library wave module combined with io.BytesIO. This approach avoids disk access and minimizes dependencies.11 A dedicated synchronous utility function (
create_wav_header) will be required to wrap the raw PCM data with the necessary WAV header information (number of channels: 1, sample width: 2 bytes, frame rate: 16000 Hz) and produce a complete WAV byte array, ready for the Vosk worker.


C. Outbound Audio Streaming and Responsiveness


The Text-to-Speech worker returns a single, complete block of WAV audio bytes corresponding to the LLM's full response. While this is generated synchronously, the transmission must be asynchronous to maintain server availability and provide a smooth, low-latency perceived playback on the client side. The WebSocket handler receives these bytes and streams them back to the client by iterating over fixed-size chunks (e.g., 4096 bytes) and using await websocket.send_bytes(chunk).13 This chunking ensures that the server is not blocked waiting for the entire audio file to transfer and maintains a continuous stream.


III. Speech-to-Text (STT) Subsystem: Vosk Worker


The STT subsystem focuses on loading the Vosk model efficiently and executing the blocking recognition process in isolation.


A. Vosk Model Initialization and Resource Strategy


Vosk models are large resources that introduce significant latency if loaded repeatedly. The architecture mandates that the Vosk Model object be initialized once globally during the application startup phase using the path specified in the environment configuration. This loaded model instance will then be accessible by the synchronous worker function, eliminating model reload overhead on every transcription request.


B. The Synchronous Transcription Function (core/stt_worker.py)


The core transcription logic is encapsulated in a synchronous function, process_audio_for_transcription. This function receives the pre-encapsulated WAV bytes, creates a KaldiRecognizer instance, and processes the audio data through it.
The function must adhere strictly to the def signature to ensure FastAPI schedules it in the internal thread pool.5 The overall execution sequence for transcription involves:
1. The asynchronous WebSocket handler receives the EOS signal.
2. The handler extracts the buffered PCM bytes.
3. The handler schedules the synchronous process_audio_for_transcription function using an explicit thread offload mechanism (e.g., asyncio.to_thread).
4. Inside the worker thread, the function calls the in-memory WAV encapsulation utility to generate the streamable WAV bytes.
5. The Vosk recognition is performed synchronously on the WAV stream.
6. The resulting transcript text is returned to the asynchronous handler.


IV. Large Language Model and Context Management Layer


The system utilizes Groq Cloud for LLM inference, requiring a focused approach to asynchronous client management to maximize throughput and minimize network latency.


A. Groq Async Client Management


Groq’s performance benefits must be leveraged through asynchronous operations. The httpx library provides the necessary AsyncClient.14 To maximize the efficiency of connection pooling and resource reuse, the
AsyncClient must not be instantiated within the per-request LLM function. Instantiating a new client in a "hot loop" or per request negates the benefits of pooling.15
Instead, the client must be managed globally via FastAPI's event handlers:
* A client instance must be created and stored in the application state during the startup event.
* The client must be explicitly closed using await client.aclose() during the shutdown event to ensure clean resource release and avoid connection leaks.14

The function interacting with the Groq API, get_llm_response, will then use this pooled, asynchronous client instance.


B. Conversation Context Store


Maintaining conversation state is crucial for coherent dialogue.16 For the prototype, the chat history is managed using a simple in-memory Python dictionary (
SESSION_CONTEXTS), keyed by the incoming session_id.17 Each entry will store a list of messages formatted for the Groq API (e.g.,
{"role": "user"/"assistant", "content": "..."}).
Table 3: Conversation Context Management Schema (In-Memory Store)
Field Name
	Data Type
	Description
	Access Requirements
	session_id
	String (UUID/str)
	Unique key derived from the WebSocket connection
	Primary key for storage retrieval
	history
	List
	Chronological list of {"role": "user"/"assistant", "content": "..."} messages
	Context window for the LLM API call
	last_activity_ts
	Float (Timestamp)
	Time of the last update/message
	Placeholder for eventual session timeout logic
	The get_llm_response function will be responsible for retrieving the existing history, appending the new user transcript, constructing the final API payload, and subsequently updating the history with the Groq response before returning the text. Token window management, while not complex in the prototype, will dictate limiting the context (e.g., keeping only the last 10 turns) to prevent context window overflow.18
It is acknowledged that this in-memory approach is process-unsafe when deploying Uvicorn with multiple workers. In a production scenario, this dependency would necessitate an external persistence layer, such as Redis or a PostgreSQL database, to ensure chat history continuity across all processes.17


V. Text-to-Speech (TTS) Subsystem: pyttsx3 Worker


The pyttsx3 component presents a significant integration challenge due to its highly synchronous nature and tendency to block the execution thread.19 The engine's fundamental operation,
runAndWait(), requires exclusive control until synthesis is complete, necessitating absolute isolation from the main event loop.20


A. pyttsx3 Isolation and Forced File Generation


The entire TTS process must be encapsulated in a synchronous worker function, synthesize_response_audio, which is explicitly scheduled in the background thread pool. Since pyttsx3 does not reliably support direct in-memory byte output across all supported platform backends, the validated approach for the prototype relies on temporary file I/O within the isolated thread.21
The worker function must perform the following sequence:
   1. Initialize the pyttsx3.init() engine locally within the thread.
   2. Use the tempfile module to securely create a temporary file path for the WAV output.
   3. Call engine.save_to_file(text, temp_path).
   4. Critically, execute engine.runAndWait() to block the worker thread until the synthesis is complete and the audio data has been written to temp_path.20
   5. Read the complete WAV audio bytes from temp_path into memory.
   6. Delete the temporary file (os.remove(temp_path)).
   7. Return the WAV bytes to the asynchronous WebSocket handler.
While this file-based approach introduces a small amount of disk I/O latency, it is an essential architectural compromise to integrate the blocking pyttsx3 library reliably within the FastAPI framework. If high latency is observed in the TTS path, this component is the highest priority for replacement with a non-blocking alternative or cloud service.


B. TTS Data Flow and Streaming


The overall conversational flow relies on the successful integration of all asynchronous and synchronous elements:
   1. LLM returns text response (async).
   2. WebSocket handler schedules synthesize_response_audio in the thread pool (sync worker).
   3. The worker thread generates and returns the complete WAV byte array.
   4. The WebSocket handler receives the bytes and begins streaming them back to the client in small chunks (async send_bytes).


VI. The Validated AI Agent Prompt Blueprint


The AI Agent is tasked with generating the complete file structure and code based on the established architectural and concurrency rules. The following prompt structure provides explicit, validated instructions for all components.


A. Setup and Environment Configuration Directive


The AI Agent must first define the necessary environment and dependencies.


1. requirements.txt


The file must contain the following dependencies to support all components:






fastapi
uvicorn[standard]
python-dotenv
websockets
vosk
pyttsx3
groq
httpx
pydantic



2. .env Configuration File


The configuration must mandate external service access credentials and resource locations:






GROQ_API_KEY="your_groq_api_key"
VOSK_MODEL_PATH="./path/to/vosk/model"
SERVER_HOST="0.0.0.0"
SERVER_PORT=8000



3. Uvicorn Launch Command


The agent must be instructed to provide the command necessary for high-concurrency operation:


Bash




uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4



B. File Structure Mandate


The Agent must structure the code into modular files under a main directory hotpin_prototype:






/hotpin_prototype
├──.env
├── requirements.txt
├── main.py                 # FastAPI application and WebSocket endpoint
└── core/
   ├── __init__.py
   ├── llm_client.py       # Groq Async Client and Context Manager
   ├── stt_worker.py       # Vosk sync worker
   └── tts_worker.py       # pyttsx3 sync worker



C. Core Module Code Generation Directives




1. Directive for core/llm_client.py


   * Imports: os, time, httpx, typing, dotenv.
   * Context Store: Define SESSION_CONTEXTS: typing.Dict[str, dict] = {} for in-memory session history storage (Table 3).
   * Groq Client Instance: Declare a variable groq_client: typing.Optional[httpx.AsyncClient] = None.
   * Functions:
   * def init_client(): Function to initialize groq_client with base URL and authorization header using the GROQ_API_KEY.
   * async def close_client(): Function to gracefully close the client instance using await groq_client.aclose().
   * def manage_context(session_id: str, role: str, content: str, max_history_turns: int = 10) -> None: Function to retrieve history, append the new message, and enforce the context window limit (max 10 turns).
   * async def get_llm_response(session_id: str, transcript: str) -> str: The core function must retrieve context, construct the API payload (including a system prompt defining the AI persona), make the request using await groq_client.post(...), update context with the LLM response, and return the response text.


2. Directive for core/stt_worker.py


   * Imports: os, wave, io, vosk, json.
   * Global Initialization: Define a global variable VOSK_MODEL and initialize it upon import using Model(os.getenv('VOSK_MODEL_PATH')).
   * Utility Function:
   * def create_wav_header(pcm_data: bytes, sample_rate: int = 16000, channels: int = 1, sample_width: int = 2) -> bytes: This function must use io.BytesIO and the standard wave module to encapsulate the raw PCM data with the correct header structure (16-bit, 16kHz, mono).11 It must return the complete WAV byte array.
   * Core Function (Synchronous):
   * def process_audio_for_transcription(session_id: str, pcm_bytes: bytes) -> str: This function must first call create_wav_header to get the WAV bytes, then initialize a KaldiRecognizer using the global VOSK_MODEL, feed the WAV data stream into it, perform the blocking recognition, and return the final transcript string.


3. Directive for core/tts_worker.py


   * Imports: pyttsx3, os, tempfile, shutil.
   * Core Function (Synchronous):
   * def synthesize_response_audio(text: str) -> bytes:
   1. Initialize engine = pyttsx3.init() inside the function scope.
   2. Set properties: engine.setProperty('rate', 175).
   3. Generate a unique temporary WAV file path using tempfile.mkstemp(suffix=".wav").
   4. Call engine.save_to_file(text, temp_path).
   5. Crucially, call engine.runAndWait() to execute synthesis and save the file.20
   6. Read the content: with open(temp_path, 'rb') as f: wav_bytes = f.read().
   7. Clean up: os.remove(temp_path).
   8. Return wav_bytes.


4. Directive for main.py (Orchestration Layer)


   * Imports: fastapi, WebSocket, WebSocketDisconnect, asyncio, typing, and relative imports for all core functions defined above.
   * App Definition: app = FastAPI().
   * Resource Management: Implement FastAPI event handlers:
   * @app.on_event("startup"): Call init_client().
   * @app.on_event("shutdown"): Call await close_client().
   * In-Memory Buffer: Define a dictionary to hold the raw audio buffers for active sessions: SESSION_AUDIO_BUFFERS: typing.Dict = {}.
   * WebSocket Endpoint:
   * @app.websocket("/ws") async def websocket_endpoint(websocket: WebSocket):
   * Connection and Handshake: Await websocket.accept(). Await the first message (Text/JSON) and extract the session_id. Initialize SESSION_AUDIO_BUFFERS[session_id] with a new io.BytesIO().
   * Main Loop: Implement a while True: loop handling incoming messages:
   1. If receive_bytes: Append data to the corresponding session's SESSION_AUDIO_BUFFERS.
   2. If receive_text (and content is the EOS signal):
a. Extract the full PCM buffer: pcm_data = SESSION_AUDIO_BUFFERS[session_id].getvalue().
b. Schedule STT in the thread pool: transcript = await asyncio.to_thread(process_audio_for_transcription, session_id, pcm_data).
c. Call LLM: llm_text = await get_llm_response(session_id, transcript).
d. Schedule TTS in the thread pool: wav_bytes = await asyncio.to_thread(synthesize_response_audio, llm_text).
e. Stream WAV output: Loop through wav_bytes in 4096-byte chunks and send via await websocket.send_bytes(chunk).
f. Reset buffer: Clear the session’s audio buffer for the next utterance.
      * Cleanup: Handle WebSocketDisconnect and remove the session's entries from both SESSION_AUDIO_BUFFERS and the context store (SESSION_CONTEXTS).


VII. Conclusions and Architectural Validation Summary


The successful construction of the Hotpin conversation prototype hinges on the meticulous segregation of synchronous, CPU-bound tasks from the asynchronous event loop. The proposed prompt blueprint mandates this separation by defining dedicated worker modules for Vosk (STT) and pyttsx3 (TTS) that rely on FastAPI's capability to execute standard def functions within its thread pool.5
The design validates the integration of critical components:
      1. Concurrency Isolation: The architecture guarantees the responsiveness of the ASGI server by offloading all blocking audio operations, scheduling them using asyncio.to_thread for clarity and control within the WebSocket handler.
      2. Resource Optimization: The use of an application-scoped httpx.AsyncClient ensures connection pooling for the low-latency Groq API.15 Furthermore, Vosk model loading is performed only once at startup, reducing per-request latency.
      3. Real-Time Data Handling: Inbound audio is handled efficiently via an in-memory PCM buffer, followed by encapsulation into the WAV format using the standard library wave utility, avoiding unnecessary disk I/O during the STT preparation phase.11 Outbound audio is streamed in chunks to provide low perceived latency to the client.13
While the prototype uses an in-memory context store and temporary file I/O for TTS, these choices prioritize functional stability and dependency simplicity. The blueprint acknowledges the need for external process-safe storage (e.g., Redis/PostgreSQL) and a non-blocking TTS solution if the system is scaled beyond the initial multi-worker development deployment.16 The validated prompt provides all necessary instructions to build a performant, modular, and robust real-time conversational server.


Final Prompt Blueprint Structure


Table 4: Final Prompt Blueprint Core Modules and Responsibilities
Module
	Concurrency Type
	Key Functions/Components
	Primary Role
	llm_client.py
	Asynchronous (I/O)
	AsyncClient, SESSION_CONTEXTS
	State management and low-latency API communication
	stt_worker.py
	Synchronous (CPU)
	VOSK_MODEL, create_wav_header
	Audio preparation and blocking transcription
	tts_worker.py
	Synchronous (CPU)
	pyttsx3.init(), tempfile usage
	Blocking audio synthesis and byte capture
	main.py
	Async Orchestration
	@app.websocket, asyncio.to_thread
	Session handling, concurrency transition, and byte streaming
	Works cited
      1. Concurrency and async / await - FastAPI, accessed on October 5, 2025, https://fastapi.tiangolo.com/async/
      2. Unleash the Power of FastAPI: Async vs Blocking I/O - DEV Community, accessed on October 5, 2025, https://dev.to/kfir-g/unleash-the-power-of-fastapi-async-vs-blocking-io-4h0b
      3. Making FastAPI Fast: A Beginner's Guide to Workers and Threads | by Cilia Madani, accessed on October 5, 2025, https://medium.com/@ciliaMadani/making-fastapi-fast-a-beginners-guide-to-workers-and-threads-095d7f2b9575
      4. Build a non-blocking API with FastAPI | by SolonSef - Medium, accessed on October 5, 2025, https://medium.com/@SolonSef/build-a-non-blocking-api-with-fastapi-6211b89c0724
      5. FastAPI is blocked when an endpoint takes longer - Reddit, accessed on October 5, 2025, https://www.reddit.com/r/FastAPI/comments/1euhq69/fastapi_is_blocked_when_an_endpoint_takes_longer/
      6. Server Workers - Uvicorn with Workers - FastAPI, accessed on October 5, 2025, https://fastapi.tiangolo.com/deployment/server-workers/
      7. WebSockets - FastAPI, accessed on October 5, 2025, https://fastapi.tiangolo.com/advanced/websockets/
      8. Using raw PCM 16 bit signed integer audio instead of file · openai whisper · Discussion #1705 - GitHub, accessed on October 5, 2025, https://github.com/openai/whisper/discussions/1705
      9. How to convert audio bytes generated by mediaRecorder and transfered using websocket to python to numpy array - Stack Overflow, accessed on October 5, 2025, https://stackoverflow.com/questions/74176807/how-to-convert-audio-bytes-generated-by-mediarecorder-and-transfered-using-webso
      10. Creating .wav file from bytes - python - Stack Overflow, accessed on October 5, 2025, https://stackoverflow.com/questions/52369925/creating-wav-file-from-bytes
      11. How to convert .pcm files to .wav files (scripting) [closed] - Stack Overflow, accessed on October 5, 2025, https://stackoverflow.com/questions/16111038/how-to-convert-pcm-files-to-wav-files-scripting
      12. Reading and Writing WAV Files in Python, accessed on October 5, 2025, https://realpython.com/python-wav-files/
      13. Websockets bridge for audio stream in FastAPI - Stack Overflow, accessed on October 5, 2025, https://stackoverflow.com/questions/65361686/websockets-bridge-for-audio-stream-in-fastapi
      14. Async Support - HTTPX, accessed on October 5, 2025, https://www.python-httpx.org/async/
      15. Enforcing `AsyncClient` as context-managed only? · Issue #769 · encode/httpx - GitHub, accessed on October 5, 2025, https://github.com/encode/httpx/issues/769
      16. Building Stateful Conversations with Postgres and LLMs | by Levi Stringer | Medium, accessed on October 5, 2025, https://medium.com/@levi_stringer/building-stateful-conversations-with-postgres-and-llms-e6bb2a5ff73e
      17. History based Contextualization in LLM deployed as a Backend Service — FastAPI and MongoDB - Jaswant Jonnada, accessed on October 5, 2025, https://jaswanth04.medium.com/history-based-contextualization-in-llm-deployed-as-a-backend-service-fastapi-and-5a1759fdd32e
      18. Efficient Context Management in LangChain Chatbots with Dragonfly, accessed on October 5, 2025, https://www.dragonflydb.io/blog/efficient-context-management-in-langchain-with-dragonfly
      19. How to save pyttsx3 results to MP3 or WAV file? - GeeksforGeeks, accessed on October 5, 2025, https://www.geeksforgeeks.org/python/how-to-save-pyttsx3-results-to-mp3-or-wav-file/
      20. Is there a way to save an audio file containing different voices with pyttsx3? - Stack Overflow, accessed on October 5, 2025, https://stackoverflow.com/questions/74960475/is-there-a-way-to-save-an-audio-file-containing-different-voices-with-pyttsx3
      21. How to save pyttsx3 results to MP3 or WAV file? - Tutorials Point, accessed on October 5, 2025, https://www.tutorialspoint.com/how-to-save-pyttsx3-results-to-mp3-or-wav-file